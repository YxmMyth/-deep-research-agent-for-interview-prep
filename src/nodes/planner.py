"""
Node 1: Planner

ç”Ÿæˆæœç´¢è®¡åˆ’ï¼Œå†³å®šå¦‚ä½•æœç´¢ JD å’Œé¢ç»
"""

import json
import logging
from src.state import AgentState
from src.prompts.templates import PLANNER_PROMPT, PLANNER_PROMPT_QUICK
from src.llm import call_llm_json
from src.progress_tracker import get_progress_tracker, AnalysisStage

logger = logging.getLogger(__name__)


async def planner_node(state: AgentState) -> AgentState:
    """
    ç”Ÿæˆç ”ç©¶è®¡åˆ’ï¼šä¸º JD æœç´¢å’Œé¢ç»æœç´¢åˆ†åˆ«ç”Ÿæˆå…³é”®è¯åˆ—è¡¨

    è¾“å…¥:
        - resume_content: ç”¨æˆ·ç®€å†
        - target_position: ç›®æ ‡å²—ä½
        - analysis_mode: åˆ†ææ¨¡å¼ ("quick" æˆ– "standard")

    è¾“å‡º:
        - jd_search_queries: JD æœç´¢å…³é”®è¯åˆ—è¡¨
        - interview_search_queries: é¢ç»æœç´¢å…³é”®è¯åˆ—è¡¨
    """
    logger.info("="*60)
    logger.info("ğŸ¯ PLANNER NODE STARTED")
    logger.info("="*60)

    # æ›´æ–°è¿›åº¦
    tracker = get_progress_tracker()
    tracker.update_stage(AnalysisStage.PLANNER)
    logger.info("âœ… Progress updated to PLANNER stage")

    # è·å–åˆ†ææ¨¡å¼
    analysis_mode = state.get("analysis_mode", "standard")
    logger.info(f"Analysis mode: {analysis_mode}")

    # æ¸…ç†æ–‡æœ¬ï¼Œç¡®ä¿æœ‰æ•ˆçš„ UTF-8
    target_position = state["target_position"]
    resume_content = state["resume_content"]

    if isinstance(target_position, str):
        target_position = target_position.encode('utf-8', errors='ignore').decode('utf-8')
    if isinstance(resume_content, str):
        resume_content = resume_content.encode('utf-8', errors='ignore').decode('utf-8')

    # æ ¹æ®åˆ†ææ¨¡å¼é€‰æ‹© prompt æ¨¡æ¿
    if analysis_mode == "quick":
        logger.info("Using QUICK mode prompt (2 queries each)")
        prompt_template = PLANNER_PROMPT_QUICK
    else:
        logger.info("Using STANDARD mode prompt (3-5 queries each)")
        prompt_template = PLANNER_PROMPT

    # æ„å»º prompt
    prompt = prompt_template.format(
        target_position=target_position,
        resume_content=resume_content,
    )

    try:
        logger.info("ğŸ“ Calling LLM API to generate search queries...")
        logger.info(f"   - Model: glm-4.7")
        logger.info(f"   - Temperature: 0.0")
        logger.info(f"   - Prompt length: {len(prompt)} chars")

        # ä½¿ç”¨æ™ºè°± AI GLM-4 API
        result = await call_llm_json(prompt, model="glm-4.7", temperature=0.0)

        logger.info("âœ… LLM API call completed successfully")
        jd_queries = result.get("jd_search_queries", [])
        interview_queries = result.get("interview_search_queries", [])

        logger.info(f"âœ… Generated {len(jd_queries)} JD queries and {len(interview_queries)} interview queries")
        logger.info(f"   - JD queries: {jd_queries}")
        logger.info(f"   - Interview queries: {interview_queries}")

        return {
            "jd_search_queries": jd_queries,
            "interview_search_queries": interview_queries,
        }

    except Exception as e:
        logger.error(f"Failed to parse planner response: {e}")
        # å›é€€åˆ°é»˜è®¤æŸ¥è¯¢
        default_jd_queries = [
            f"{target_position} æ‹›è˜",
            f"{target_position} job description",
        ]
        default_interview_queries = [
            f"{target_position} é¢ç»",
            f"{target_position} interview experience",
        ]

        return {
            "jd_search_queries": default_jd_queries,
            "interview_search_queries": default_interview_queries,
        }
