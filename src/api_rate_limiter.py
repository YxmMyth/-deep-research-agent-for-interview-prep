"""
API é€Ÿç‡é™åˆ¶å™¨ - ZhipuAI API å¹¶å‘æ§åˆ¶å’Œé‡è¯•æœºåˆ¶

åŠŸèƒ½:
- ä½¿ç”¨ asyncio.Semaphore æ§åˆ¶æœ€å¤§å¹¶å‘æ•°
- è‡ªåŠ¨æ£€æµ‹ 429 é”™è¯¯å¹¶é‡è¯•
- æŒ‡æ•°é€€é¿é‡è¯•ç­–ç•¥
- ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½
- çº¿ç¨‹å±€éƒ¨å­˜å‚¨ï¼Œé¿å…è·¨äº‹ä»¶å¾ªç¯çš„ Semaphore å…±äº«é—®é¢˜
"""

import asyncio
import os
import logging
import threading
import inspect
from typing import Callable, Any, Optional

logger = logging.getLogger(__name__)

# å…¨å±€å•ä¾‹æ¨¡å¼ï¼ˆæ‰€æœ‰çº¿ç¨‹å…±äº«åŒä¸€ä¸ªé™æµå™¨å®ä¾‹ï¼‰
_global_rate_limiter = None
_limiter_lock = threading.Lock()
_global_adaptive_rate_limiter = None
_adaptive_limiter_lock = threading.Lock()


class APIRateLimiter:
    """
    ZhipuAI API é€Ÿç‡é™åˆ¶å™¨

    ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘ API è°ƒç”¨æ•°é‡ï¼Œé˜²æ­¢è§¦å‘ 429 é”™è¯¯ã€‚
    é‡åˆ°é€Ÿç‡é™åˆ¶æ—¶è‡ªåŠ¨ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥é‡è¯•ã€‚
    """

    def __init__(
        self,
        max_concurrent: Optional[int] = None,
        max_retries: Optional[int] = None,
        initial_backoff: Optional[float] = None
    ):
        """
        åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨

        Args:
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæˆ–ä½¿ç”¨ 3ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæˆ–ä½¿ç”¨ 3ï¼‰
            initial_backoff: åˆå§‹é€€é¿æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæˆ–ä½¿ç”¨ 1.0ï¼‰
        """
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.max_concurrent = max_concurrent or int(os.getenv("ZHIPUAI_MAX_CONCURRENT", "3"))
        self.max_retries = max_retries or int(os.getenv("ZHIPUAI_MAX_RETRIES", "3"))
        self.initial_backoff = initial_backoff or float(os.getenv("ZHIPUAI_INITIAL_BACKOFF", "1.0"))

        # å»¶è¿Ÿåˆå§‹åŒ–ä¿¡å·é‡ï¼ˆä¸åœ¨ __init__ ä¸­åˆ›å»ºï¼Œé¿å…ç»‘å®šåˆ°é”™è¯¯çš„äº‹ä»¶å¾ªç¯ï¼‰
        self._semaphore = None
        self._semaphore_loop = None

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "rate_limited_calls": 0,
            "retries": 0,
            "failed_calls": 0
        }

        logger.info(
            f"API Rate Limiter initialized: max_concurrent={self.max_concurrent}, "
            f"max_retries={self.max_retries}, initial_backoff={self.initial_backoff}s"
        )

    async def _get_or_create_semaphore(self):
        """
        åœ¨å½“å‰äº‹ä»¶å¾ªç¯ä¸­åˆ›å»ºæˆ–è·å– Semaphore

        è¿™æ˜¯è§£å†³è·¨äº‹ä»¶å¾ªç¯é—®é¢˜çš„å…³é”®ï¼šSemaphore åœ¨å®é™…ä½¿ç”¨æ—¶åˆ›å»ºï¼Œ
        è€Œä¸æ˜¯åœ¨ __init__ ä¸­åˆ›å»ºï¼Œå› æ­¤ä¼šç»‘å®šåˆ°å½“å‰çš„äº‹ä»¶å¾ªç¯ã€‚
        åŒæ—¶æ£€æŸ¥å½“å‰ loop æ˜¯å¦ä¸åˆ›å»º semaphore çš„ loop ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™é‡å»ºã€‚
        """
        current_loop = asyncio.get_running_loop()
        if self._semaphore is None or self._semaphore_loop != current_loop:
            self._semaphore = asyncio.Semaphore(self.max_concurrent)
            self._semaphore_loop = current_loop
            logger.debug(f"Created new Semaphore in current event loop (id: {id(current_loop)})")
        return self._semaphore

    async def __aenter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè·å–ä¿¡å·é‡"""
        logger.info(f"ğŸ”’ Rate limiter: Attempting to acquire semaphore...")
        sem = await self._get_or_create_semaphore()
        logger.info(f"ğŸ”‘ Rate limiter: Semaphore created/retrieved, waiting to acquire...")
        await sem.acquire()
        self._stats["total_calls"] += 1
        active = self.max_concurrent - sem._value
        logger.info(f"âœ… Rate limiter: Semaphore acquired (active: {active}/{self.max_concurrent})")
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œé‡Šæ”¾ä¿¡å·é‡"""
        sem = await self._get_or_create_semaphore()
        sem.release()
        logger.debug(f"Released semaphore (active: {self.max_concurrent - sem._value}/{self.max_concurrent})")
        return False

    async def call_with_retry(
        self,
        api_func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        æ‰§è¡Œ API è°ƒç”¨å¹¶è‡ªåŠ¨é‡è¯•

        Args:
            api_func: API è°ƒç”¨å‡½æ•°ï¼ˆå¯ä»¥æ˜¯åŒæ­¥æˆ–å¼‚æ­¥å‡½æ•°ï¼‰
            *args, **kwargs: ä¼ é€’ç»™ API å‡½æ•°çš„å‚æ•°

        Returns:
            API å“åº”ç»“æœ
        """
        last_exception = None

        for attempt in range(self.max_retries + 1):
            try:
                # æ‰§è¡Œ API è°ƒç”¨
                if inspect.iscoroutinefunction(api_func):
                    result = await api_func(*args, **kwargs)
                else:
                    result = api_func(*args, **kwargs)
                    if inspect.isawaitable(result):
                        result = await result

                # è®°å½•æˆåŠŸ
                if attempt == 0:
                    self._stats["successful_calls"] += 1

                return result

            except Exception as e:
                last_exception = e
                error_str = str(e)

                # æ£€æŸ¥æ˜¯å¦ä¸ºé€Ÿç‡é™åˆ¶é”™è¯¯
                if is_rate_limit_error(e) and attempt < self.max_retries:
                    # è®¡ç®—é€€é¿æ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                    backoff_time = self.initial_backoff * (2 ** attempt)
                    self._stats["rate_limited_calls"] += 1
                    self._stats["retries"] += 1

                    logger.warning(
                        f"Rate limited (attempt {attempt + 1}/{self.max_retries + 1}), "
                        f"waiting {backoff_time:.1f}s before retry... "
                        f"Error: {error_str[:200]}"
                    )

                    # ç­‰å¾…åé‡è¯•
                    await asyncio.sleep(backoff_time)
                    continue
                else:
                    # é 429 é”™è¯¯æˆ–é‡è¯•æ¬¡æ•°ç”¨å°½
                    if attempt < self.max_retries:
                        logger.error(f"API call failed (non-rate-limit error): {error_str[:200]}")
                    else:
                        logger.error(f"Max retries ({self.max_retries}) exceeded: {error_str[:200]}")

                    self._stats["failed_calls"] += 1
                    raise

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†ç±»å‹å®‰å…¨
        if last_exception:
            raise last_exception

    def get_stats(self) -> dict:
        """
        è·å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«ç»Ÿè®¡æ•°æ®çš„å­—å…¸
        """
        return self._stats.copy()

    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self._stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "rate_limited_calls": 0,
            "retries": 0,
            "failed_calls": 0
        }


def is_rate_limit_error(error: Exception) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦ä¸ºé€Ÿç‡é™åˆ¶é”™è¯¯ï¼ˆ429 æˆ–ç›¸å…³é”™è¯¯ç ï¼‰

    Args:
        error: å¼‚å¸¸å¯¹è±¡

    Returns:
        å¦‚æœæ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    error_str = str(error).lower()

    # æ£€æŸ¥å¤šç§é€Ÿç‡é™åˆ¶ç›¸å…³çš„æ ‡è¯†
    rate_limit_indicators = [
        "429",  # HTTP 429 çŠ¶æ€ç 
        "1302",  # ZhipuAI å¹¶å‘é™åˆ¶é”™è¯¯ç 
        "rate limit",  # é€šç”¨é€Ÿç‡é™åˆ¶
        "å¹¶å‘",  # ä¸­æ–‡"å¹¶å‘"
        "concurrent",  # è‹±æ–‡"å¹¶å‘"
        "quota",  # é…é¢é™åˆ¶
        "too many requests",  # è¯·æ±‚è¿‡å¤š
    ]

    return any(indicator in error_str for indicator in rate_limit_indicators)


def get_api_rate_limiter() -> APIRateLimiter:
    """
    è·å–å…¨å±€å”¯ä¸€çš„é€Ÿç‡é™åˆ¶å™¨å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

    æ‰€æœ‰çº¿ç¨‹å…±äº«åŒä¸€ä¸ªé™æµå™¨å®ä¾‹ï¼Œç¡®ä¿å…¨å±€å¹¶å‘é™åˆ¶çœŸæ­£ç”Ÿæ•ˆã€‚
    ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ä¿è¯çº¿ç¨‹å®‰å…¨çš„åˆå§‹åŒ–ã€‚

    Returns:
        APIRateLimiter å®ä¾‹
    """
    global _global_rate_limiter
    if _global_rate_limiter is None:
        with _limiter_lock:
            if _global_rate_limiter is None:
                _global_rate_limiter = APIRateLimiter()
                logger.info("Created global APIRateLimiter instance (shared across all threads)")
    return _global_rate_limiter


class AdaptiveAPIRateLimiter(APIRateLimiter):
    """
    è‡ªé€‚åº” API é€Ÿç‡é™åˆ¶å™¨

    åœ¨åŸºç¡€é€Ÿç‡é™åˆ¶å™¨ä¹‹ä¸Šæ·»åŠ è‡ªé€‚åº”å¹¶å‘æ§åˆ¶ï¼š
    - æ£€æµ‹é¢‘ç¹çš„ 429 é”™è¯¯å¹¶è‡ªåŠ¨é™ä½å¹¶å‘æ•°
    - æ”¯æŒæ‰‹åŠ¨å¯ç”¨ç›‘æ§æ¨¡å¼ï¼ˆé™ä½å¹¶å‘ä»¥é¿å…ä¸å…¶ä»–åº”ç”¨å†²çªï¼‰
    - åŠ¨æ€è°ƒæ•´å¹¶å‘é™åˆ¶ä»¥ä¼˜åŒ–æ€§èƒ½
    """

    def __init__(
        self,
        max_concurrent: Optional[int] = None,
        max_retries: Optional[int] = None,
        initial_backoff: Optional[float] = None
    ):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”é€Ÿç‡é™åˆ¶å™¨

        Args:
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæˆ–ä½¿ç”¨ 3ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæˆ–ä½¿ç”¨ 3ï¼‰
            initial_backoff: åˆå§‹é€€é¿æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œæˆ–ä½¿ç”¨ 1.0ï¼‰
        """
        super().__init__(max_concurrent, max_retries, initial_backoff)

        # è‡ªé€‚åº”æ§åˆ¶å‚æ•°
        self._current_concurrent_limit = self.max_concurrent  # å½“å‰æœ‰æ•ˆçš„å¹¶å‘é™åˆ¶
        self._recent_429_count = 0  # æœ€è¿‘çš„ 429 é”™è¯¯è®¡æ•°
        self._429_threshold = 3  # è§¦å‘é™çº§çš„ 429 é”™è¯¯é˜ˆå€¼
        self._last_429_time = None  # ä¸Šæ¬¡ 429 é”™è¯¯çš„æ—¶é—´

        logger.info(
            f"Adaptive API Rate Limiter initialized: max_concurrent={self.max_concurrent}, "
            f"adaptive_control enabled (429_threshold={self._429_threshold})"
        )

    def enable_monitoring_mode(self):
        """
        å¯ç”¨ç›‘æ§æ¨¡å¼ï¼ˆé™ä½å¹¶å‘æ•°ï¼‰

        å½“æ£€æµ‹åˆ°ä¸å…¶ä»–åº”ç”¨ï¼ˆå¦‚ç›‘æ§åº”ç”¨ï¼‰çš„å¹¶å‘å†²çªæ—¶ï¼Œ
        å¯ä»¥æ‰‹åŠ¨è°ƒç”¨æ­¤æ–¹æ³•é™ä½å¹¶å‘é™åˆ¶ã€‚
        """
        old_limit = self._current_concurrent_limit
        self._current_concurrent_limit = max(1, self.max_concurrent // 2)
        self._semaphore = None  # é‡æ–°åˆ›å»ºä¿¡å·é‡ä»¥åº”ç”¨æ–°é™åˆ¶

        logger.warning(
            f"âš ï¸ Monitoring mode enabled. Concurrent limit reduced: "
            f"{old_limit} â†’ {self._current_concurrent_limit}"
        )

    async def _get_or_create_semaphore(self):
        """
        åœ¨å½“å‰äº‹ä»¶å¾ªç¯ä¸­åˆ›å»ºæˆ–è·å– Semaphoreï¼ˆä½¿ç”¨è‡ªé€‚åº”å¹¶å‘é™åˆ¶ï¼‰
        """
        current_loop = asyncio.get_running_loop()
        if self._semaphore is None or self._semaphore_loop != current_loop:
            self._semaphore = asyncio.Semaphore(self._current_concurrent_limit)
            self._semaphore_loop = current_loop
            logger.debug(
                f"Created new Semaphore with adaptive limit: {self._current_concurrent_limit} "
                f"(base: {self.max_concurrent}) in loop {id(current_loop)}"
            )
        return self._semaphore

    async def call_with_retry(
        self,
        api_func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """
        æ‰§è¡Œ API è°ƒç”¨å¹¶è‡ªåŠ¨é‡è¯•ï¼ˆå¸¦è‡ªé€‚åº”å¹¶å‘æ§åˆ¶ï¼‰

        å½“æ£€æµ‹åˆ°é¢‘ç¹çš„ 429 é”™è¯¯æ—¶ï¼Œè‡ªåŠ¨é™ä½å¹¶å‘é™åˆ¶ã€‚

        Args:
            api_func: API è°ƒç”¨å‡½æ•°
            *args, **kwargs: ä¼ é€’ç»™ API å‡½æ•°çš„å‚æ•°

        Returns:
            API å“åº”ç»“æœ

        Raises:
            Exception: é‡è¯•æ¬¡æ•°ç”¨å°½åä»å¤±è´¥æ—¶æŠ›å‡ºåŸå§‹å¼‚å¸¸
        """
        last_exception = None

        for attempt in range(self.max_retries + 1):
            try:
                # æ‰§è¡Œ API è°ƒç”¨
                result = api_func(*args, **kwargs)

                # è®°å½•æˆåŠŸ
                if attempt == 0:
                    self._stats["successful_calls"] += 1

                # å¦‚æœä¹‹å‰æœ‰ 429 é”™è¯¯ï¼Œé€æ­¥æ¢å¤å¹¶å‘é™åˆ¶
                if self._recent_429_count > 0 and attempt == 0:
                    self._gradually_restore_concurrent_limit()

                return result

            except Exception as e:
                last_exception = e
                error_str = str(e)

                # æ£€æŸ¥æ˜¯å¦ä¸ºé€Ÿç‡é™åˆ¶é”™è¯¯
                if is_rate_limit_error(e):
                    # è®°å½• 429 é”™è¯¯
                    self._on_rate_limit_error()

                    if attempt < self.max_retries:
                        # è®¡ç®—é€€é¿æ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
                        backoff_time = self.initial_backoff * (2 ** attempt)
                        self._stats["rate_limited_calls"] += 1
                        self._stats["retries"] += 1

                        logger.warning(
                            f"Rate limited (attempt {attempt + 1}/{self.max_retries + 1}), "
                            f"waiting {backoff_time:.1f}s before retry... "
                            f"Error: {error_str[:200]}"
                        )

                        # ç­‰å¾…åé‡è¯•
                        await asyncio.sleep(backoff_time)
                        continue
                    else:
                        # é‡è¯•æ¬¡æ•°ç”¨å°½
                        logger.error(
                            f"Max retries ({self.max_retries}) exceeded: {error_str[:200]}"
                        )
                        self._stats["failed_calls"] += 1
                        raise
                else:
                    # é 429 é”™è¯¯
                    if attempt < self.max_retries:
                        logger.error(f"API call failed (non-rate-limit error): {error_str[:200]}")
                    else:
                        logger.error(f"Max retries ({self.max_retries}) exceeded: {error_str[:200]}")

                    self._stats["failed_calls"] += 1
                    raise

        # ç†è®ºä¸Šä¸ä¼šåˆ°è¾¾è¿™é‡Œï¼Œä½†ä¸ºäº†ç±»å‹å®‰å…¨
        if last_exception:
            raise last_exception

    def _on_rate_limit_error(self):
        """
        å¤„ç†é€Ÿç‡é™åˆ¶é”™è¯¯

        å½“æ£€æµ‹åˆ° 429 é”™è¯¯æ—¶ï¼Œå¢åŠ è®¡æ•°å¹¶æ£€æŸ¥æ˜¯å¦éœ€è¦é™ä½å¹¶å‘é™åˆ¶ã€‚
        """
        import time

        self._recent_429_count += 1
        current_time = time.time()

        # æ£€æŸ¥æ˜¯å¦åœ¨çŸ­æ—¶é—´å†…é¢‘ç¹é‡åˆ° 429
        if self._last_429_time and (current_time - self._last_429_time) < 30:
            # 30 ç§’å†…å¤šæ¬¡é‡åˆ° 429ï¼Œéœ€è¦é™ä½å¹¶å‘
            if self._recent_429_count >= self._429_threshold:
                old_limit = self._current_concurrent_limit
                self._current_concurrent_limit = max(1, self._current_concurrent_limit - 1)
                self._semaphore = None  # é‡æ–°åˆ›å»ºä¿¡å·é‡ä»¥åº”ç”¨æ–°é™åˆ¶

                logger.warning(
                    f"âš ï¸ Frequent 429 errors detected. Reducing concurrent limit: "
                    f"{old_limit} â†’ {self._current_concurrent_limit} "
                    f"(429 count: {self._recent_429_count})"
                )

                # é‡ç½®è®¡æ•°å™¨ä»¥é¿å…è¿‡åº¦é™çº§
                self._recent_429_count = 0
        else:
            # è·ç¦»ä¸Šæ¬¡ 429 è¾ƒä¹…ï¼Œé‡ç½®è®¡æ•°
            self._recent_429_count = 1

        self._last_429_time = current_time

    def _gradually_restore_concurrent_limit(self):
        """
        é€æ­¥æ¢å¤å¹¶å‘é™åˆ¶

        å½“ API è°ƒç”¨æˆåŠŸæ—¶ï¼Œé€æ­¥æ¢å¤å¹¶å‘é™åˆ¶åˆ°åŸå§‹å€¼ã€‚
        """
        if self._current_concurrent_limit < self.max_concurrent:
            self._current_concurrent_limit = min(
                self.max_concurrent,
                self._current_concurrent_limit + 1
            )
            self._semaphore = None  # é‡æ–°åˆ›å»ºä¿¡å·é‡ä»¥åº”ç”¨æ–°é™åˆ¶

            logger.info(
                f"âœ… Restoring concurrent limit: {self._current_concurrent_limit} "
                f"(target: {self.max_concurrent})"
            )

    def get_adaptive_stats(self) -> dict:
        """
        è·å–è‡ªé€‚åº”ç»Ÿè®¡ä¿¡æ¯

        Returns:
            åŒ…å«åŸºç¡€ç»Ÿè®¡å’Œè‡ªé€‚åº”æ§åˆ¶æ•°æ®çš„å­—å…¸
        """
        stats = self.get_stats()
        stats.update({
            "current_concurrent_limit": self._current_concurrent_limit,
            "base_concurrent_limit": self.max_concurrent,
            "recent_429_count": self._recent_429_count,
            "adaptation_active": self._current_concurrent_limit < self.max_concurrent
        })
        return stats


def get_adaptive_api_rate_limiter() -> AdaptiveAPIRateLimiter:
    """
    è·å–å…¨å±€å”¯ä¸€çš„è‡ªé€‚åº”é€Ÿç‡é™åˆ¶å™¨å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

    æ‰€æœ‰çº¿ç¨‹å…±äº«åŒä¸€ä¸ªé™æµå™¨å®ä¾‹ï¼Œç¡®ä¿å…¨å±€å¹¶å‘é™åˆ¶çœŸæ­£ç”Ÿæ•ˆã€‚
    ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ä¿è¯çº¿ç¨‹å®‰å…¨çš„åˆå§‹åŒ–ã€‚

    Returns:
        AdaptiveAPIRateLimiter å®ä¾‹
    """
    global _global_adaptive_rate_limiter
    if _global_adaptive_rate_limiter is None:
        with _adaptive_limiter_lock:
            if _global_adaptive_rate_limiter is None:
                _global_adaptive_rate_limiter = AdaptiveAPIRateLimiter()
                logger.info("Created global AdaptiveAPIRateLimiter instance (shared across all threads)")
    return _global_adaptive_rate_limiter
